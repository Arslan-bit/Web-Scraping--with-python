{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from  bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import string\n",
    "from urllib.request import Request, urlopen\n",
    "import re\n",
    "import os\n",
    "from urllib.parse import urljoin\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://stackoverflow.com/questions/30042334/attribute-error-list-object-has-no-attribute-split\"\n",
    "# url = \"https://www.geeksforgeeks.org/create-a-directory-in-python/\"\n",
    "# url='https://en.wikipedia.org/wiki/Imran_Khan'\n",
    "# url='https://stackoverflow.com/questions/3711856/how-to-remove-empty-lines-with-or-without-whitespace-in-python'\n",
    "# url=\"https://www.tutorialspoint.com/downloading-files-from-web-using-python\"\n",
    "# url=\"https://www.tutorialspoint.com/beyond_basic_programming_intermediate_python/index.asp\"\n",
    "url = 'https://techfor.netlify.app'\n",
    "# url = 'https://alltop.netlify.app'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get The Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "r = requests.get(url)\n",
    "htmlcontents  = r.content\n",
    "# print(htmlcontents)\n",
    "soop = BeautifulSoup(htmlcontents,'html.parser')\n",
    "# print(soop.prettify())\n",
    "\n",
    "paragraph = soop.find_all('p')\n",
    "# print(paragraph)\n",
    "\n",
    "heading = soop.find('title')\n",
    "heaad = heading.text\n",
    "\n",
    "# heaad = heaad.split(' ')[:3] if len(heaad.split(' ')) > 1 else heaad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech4Life (Pvt.) Limited\n"
     ]
    }
   ],
   "source": [
    "print((heaad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the Heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tech4Life Pvt Limited\n"
     ]
    }
   ],
   "source": [
    "# a_string = '!hi. wh?at is the weat[h]er lik?e.'\n",
    "a_string = heaad\n",
    "new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "print(new_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Main Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Tech4Life Pvt Limited' created\n",
      "C:/Scraping/Tech4Life Pvt Limited\n"
     ]
    }
   ],
   "source": [
    "# Directory\n",
    "directory = new_string\n",
    "  \n",
    "# Parent Directory path\n",
    "parent_dir = \"C:/Scraping/\"\n",
    "  \n",
    "# Path join\n",
    "path = os.path.join(parent_dir, directory)\n",
    "  \n",
    "os.mkdir(path)\n",
    "print(\"Directory '% s' created\" % directory)\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make List Sub Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_add_dir = parent_dir + directory +\"/\"\n",
    "# print(sub_add_dir)\n",
    "list = [\"html\",\"css\",\"java\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Sub Directory in Main Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'html' created\n",
      "Directory 'css' created\n",
      "Directory 'java' created\n"
     ]
    }
   ],
   "source": [
    "n=0\n",
    "while n <len(list):\n",
    "    sub_directory =list[n] \n",
    "\n",
    "    # Path join\n",
    "    spath = os.path.join(sub_add_dir, sub_directory)\n",
    "\n",
    "    os.mkdir(spath)\n",
    "    \n",
    "    print(\"Directory '% s' created\" % sub_directory)\n",
    "    \n",
    "    n=n+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['css', 'html', 'java']\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "N_folder = os.listdir(path)\n",
    "print(N_folder)\n",
    "n = len(N_folder)\n",
    "print(n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create .txt file in the sub Directorys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Scraping/Tech4Life Pvt Limited/css/css.txt\n",
      "C:/Scraping/Tech4Life Pvt Limited/html/html.txt\n",
      "C:/Scraping/Tech4Life Pvt Limited/java/java.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n=0\n",
    "while n<len(N_folder):\n",
    "    \n",
    "    file = open(path + '/' + N_folder[n] + '/' + N_folder[n] + '.txt','w')\n",
    "    file.close\n",
    "\n",
    "    print((path + '/' + N_folder[n] + '/' + N_folder[n] + '.txt'))\n",
    "    \n",
    "    n=n+1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract & split the HTML and save in html.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HTML\n",
    "req = Request(url)\n",
    "html_page = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"html.parser\")\n",
    "\n",
    "\n",
    "html_text = soup\n",
    "\n",
    "Func = open(path + '/' + N_folder[1] + '/' + N_folder[1] + '.html',\"w\")\n",
    "\n",
    "Func.write(str(soup))\n",
    "\n",
    "Func.close()\n",
    "\n",
    "# print(str(soup))\n",
    "\n",
    "# img = soup.find(\"img\")[\"src\"]\n",
    "# print(img)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#contactus\n",
      "#contactus\n",
      "#contactus\n",
      "#contactus\n",
      "#contactus\n",
      "https://facebook.com/tech4lifeltd\n",
      "https://Twitter.com/tech4lifeltd\n",
      "https://Instagram.com/tech4lifeltd\n",
      "https://www.youtube.com/channel/UCg4HwIKVU2kQ3nTIhrSYhSw\n",
      "https://Linkedin.com/company/tech4lifeltd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "response = requests.get(url)\n",
    "# parse html\n",
    "page = str(BeautifulSoup(response.content))\n",
    "\n",
    "\n",
    "def getURL(page):\n",
    "    \"\"\"\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"a href\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "while True:\n",
    "    url, n = getURL(page)\n",
    "    page = page[n:]\n",
    "    if url:\n",
    "        print(url)\n",
    "    else:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/img/logo with name white.jpg <class 'str'>\n",
      "/img/first slide picture.jpg <class 'str'>\n",
      "/img/second slide picture.jpg <class 'str'>\n",
      "/img/third slide picture.jpg <class 'str'>\n",
      "/img/fourth slide picture.jpg <class 'str'>\n",
      "/img/solar-energy.png <class 'str'>\n",
      "/img/building.png <class 'str'>\n",
      "/img/social-care.png <class 'str'>\n",
      "/img/analyst.png <class 'str'>\n",
      "/img/about.png <class 'str'>\n",
      "/img/domestic.jpg <class 'str'>\n",
      "/img/commercial.jpg <class 'str'>\n",
      "/img/industrial.jpg <class 'str'>\n",
      "/img/5 and 10 kW.png <class 'str'>\n",
      "/img/5 and 10 kW.png <class 'str'>\n",
      "/img/20 and 30 kW.png <class 'str'>\n",
      "/img/20 and 30 kW.png <class 'str'>\n",
      "/img/Work/Agriculture Fort Abbas.jpg <class 'str'>\n",
      "/img/Work/Commercial Lahore.jpg <class 'str'>\n",
      "/img/Work/Residential Bahria Town.jpg <class 'str'>\n",
      "/img/Work/Residential Cavalry Ground.jpg <class 'str'>\n",
      "/img/Work/Residential DHA EME.jpg <class 'str'>\n",
      "/img/Work/Residential Lake City.jpg <class 'str'>\n",
      "/img/Work/Residential OPF Society.jpg <class 'str'>\n",
      "/img/Work/Residential Punjab Society.jpg <class 'str'>\n",
      "/img/Work/Residential UET Society.jpg <class 'str'>\n",
      "/img/Work/Residential Valecia Society.jpg <class 'str'>\n",
      "/img/Work/Resintial DHA Phase 1.jpg <class 'str'>\n",
      "/img/Work/Corporate Lahore.jpg <class 'str'>\n",
      "https://techfor.netlify.app/img/logo with name white.jpg\n",
      "https://techfor.netlify.app/img/first slide picture.jpg\n",
      "https://techfor.netlify.app/img/second slide picture.jpg\n",
      "https://techfor.netlify.app/img/third slide picture.jpg\n",
      "https://techfor.netlify.app/img/fourth slide picture.jpg\n",
      "https://techfor.netlify.app/img/solar-energy.png\n",
      "https://techfor.netlify.app/img/building.png\n",
      "https://techfor.netlify.app/img/social-care.png\n",
      "https://techfor.netlify.app/img/analyst.png\n",
      "https://techfor.netlify.app/img/about.png\n",
      "https://techfor.netlify.app/img/domestic.jpg\n",
      "https://techfor.netlify.app/img/commercial.jpg\n",
      "https://techfor.netlify.app/img/industrial.jpg\n",
      "https://techfor.netlify.app/img/5 and 10 kW.png\n",
      "https://techfor.netlify.app/img/5 and 10 kW.png\n",
      "https://techfor.netlify.app/img/20 and 30 kW.png\n",
      "https://techfor.netlify.app/img/20 and 30 kW.png\n",
      "https://techfor.netlify.app/img/Work/Agriculture Fort Abbas.jpg\n",
      "https://techfor.netlify.app/img/Work/Commercial Lahore.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential Bahria Town.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential Cavalry Ground.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential DHA EME.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential Lake City.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential OPF Society.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential Punjab Society.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential UET Society.jpg\n",
      "https://techfor.netlify.app/img/Work/Residential Valecia Society.jpg\n",
      "https://techfor.netlify.app/img/Work/Resintial DHA Phase 1.jpg\n",
      "https://techfor.netlify.app/img/Work/Corporate Lahore.jpg\n"
     ]
    }
   ],
   "source": [
    "# from BeautifulSoup import BeautifulSoup\n",
    "\n",
    "# import re\n",
    "html_page = urlopen(req)\n",
    "soup = BeautifulSoup(html_page)\n",
    "images = []\n",
    "for img in soup.find_all('img'):\n",
    "    src = img.get('src').replace('./', '/')\n",
    "    if not src.startswith('/'): src = f'/{src}'\n",
    "    print(src, type(src))\n",
    "    if 'http' not in src: images.append(src)\n",
    "\n",
    "# print(images)\n",
    "\n",
    "# print(images)\n",
    "# img_link = \"\"\n",
    "links = []\n",
    "for line in images:\n",
    "\n",
    "     links.append(\"https://techfor.netlify.app\"+line)\n",
    "        \n",
    "        \n",
    "for link in links: print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Extract the Java and save in java.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Java Urls\n",
    "script_files = []\n",
    "\n",
    "for script in soup.find_all(\"script\"):\n",
    "    if script.attrs.get(\"src\"):\n",
    "        # if the tag has the attribute 'src'\n",
    "        script_url = urljoin(url, script.attrs.get(\"src\"))\n",
    "        script_files.append(script_url)\n",
    "        \n",
    "        \n",
    "# change list to text   \n",
    "java_text = \"\"\n",
    "\n",
    "for line in script_files:\n",
    "\n",
    "      java_text += line + \"\\n https://techfor.netlify.app\"\n",
    "\n",
    "        \n",
    "        \n",
    "# Add java urls in File\n",
    "f = open(path + '/' + N_folder[2] + '/' + N_folder[2] + '.txt','w',encoding='utf-8')         # Creating html_text.txt File\n",
    "\n",
    "for line in java_text:\n",
    "\tf.write(line)\n",
    "\n",
    "f.close()\n",
    "# print(script_files[0])\n",
    "print(java_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "# open(path + '/' + N_folder[2] + '/' + URL[2] + '.java', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading Java files & save in Java Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = 0\n",
    "while aa < len(script_files):\n",
    "    \n",
    "    url = script_files[aa]\n",
    "    URL = url.split('/')\n",
    "\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    java_path = (URL[2] + URL[3]+ str(aa)) \n",
    "\n",
    "    open(path + '/' + N_folder[2] + '/' + java_path + '.java', 'wb').write(r.content)\n",
    "    \n",
    "    print('File Save '+ java_path + '.java' )\n",
    "    \n",
    "    aa = aa+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Extract the CSS and save in css.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CSS Urls\n",
    "css_files = []\n",
    "\n",
    "for css in soup.find_all(\"link\"):\n",
    "    if css.attrs.get(\"href\"):\n",
    "        # if the link tag has the 'href' attribute\n",
    "        css_url = urljoin(url, css.attrs.get(\"href\"))\n",
    "        css_files.append(css_url)\n",
    "        \n",
    "# change list to text       \n",
    "css_text = \"\"\n",
    "\n",
    "for line in css_files:\n",
    "\n",
    "      css_text += line + \"\\n\"\n",
    "\n",
    "\n",
    "        \n",
    "# Add css urls in File\n",
    "f = open(path + '/' + N_folder[0] + '/' + N_folder[0] + '.txt','w')         # Creating html_text.txt File\n",
    "\n",
    "for line in css_text:\n",
    "\tf.write(line)\n",
    "\n",
    "f.close()\n",
    "# print(css_files)\n",
    "print(css_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading CSS files & Img  & save in Css Directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_string = css_files[3]\n",
    "# # a_string = a_strin.split('.')\n",
    "# print(a_string)\n",
    "# new_string = a_string.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# print(new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "while a < len(css_files):\n",
    "    \n",
    "    url = css_files[a]\n",
    "    URL = url.split('/')\n",
    "\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    java_path = (URL[2] + str(a)) \n",
    "\n",
    "    \n",
    "    \n",
    "    if (URL.count(\"Img\") > 0):\n",
    "        open(path + '/' + N_folder[0] + '/' + java_path + '.png', 'wb').write(r.content)\n",
    "        print('File Save '+ java_path + '.png' )\n",
    "        \n",
    "    else: \n",
    "        open(path + '/' + N_folder[0] + '/' + java_path + '.css', 'wb').write(r.content)\n",
    "        print('File Save '+ java_path + '.css' )\n",
    "    \n",
    "    a = a+1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
